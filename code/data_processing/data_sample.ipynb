{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preview (first 5 rows):\n",
      "            Query Name                   Date  \\\n",
      "0  UnitedHealth_2month  2024-11-12 17:59:45.0   \n",
      "1  UnitedHealth_2month  2024-11-12 17:59:02.0   \n",
      "2  UnitedHealth_2month  2024-11-12 17:57:58.0   \n",
      "3  UnitedHealth_2month  2024-11-12 17:57:07.0   \n",
      "4  UnitedHealth_2month  2024-11-12 17:55:04.0   \n",
      "\n",
      "                                           Full Text       Location Name  \\\n",
      "0  RT @CBSNews With ACA subsidies set to expire i...             AZ, USA   \n",
      "1  RT @cromwick Moreover, isn't this 'rope was in...    Atlanta, GA, USA   \n",
      "2  Cute girl sparked up a conversation with me ab...  Charlotte, NC, USA   \n",
      "3  @unusual_whales Sad for those that wish they c...    Seattle, WA, USA   \n",
      "4  RT @mfcannon Dear Congress,\\n\\nThese lobbyists...             VA, USA   \n",
      "\n",
      "  Account Type           Author  Impact  Impressions  Engagement Score  \\\n",
      "0   individual  SafeH2o4Schools    36.1        13919               0.0   \n",
      "1   individual     tgordonvideo     0.7         1769               0.0   \n",
      "2   individual   mellowtoo_hype     5.6         2788               0.0   \n",
      "3   individual  NateLyn11519676     0.0          296               0.0   \n",
      "4   individual   rachel_johns0n     0.0          544               0.0   \n",
      "\n",
      "           Interest  \n",
      "0               NaN  \n",
      "1               NaN  \n",
      "2               NaN  \n",
      "3  Politics, Travel  \n",
      "4               NaN  \n",
      "\n",
      "Data shape (rows, columns):\n",
      "(702699, 10)\n",
      "\n",
      "Column names:\n",
      "['Query Name', 'Date', 'Full Text', 'Location Name', 'Account Type', 'Author', 'Impact', 'Impressions', 'Engagement Score', 'Interest']\n",
      "\n",
      "Data types:\n",
      "Query Name           object\n",
      "Date                 object\n",
      "Full Text            object\n",
      "Location Name        object\n",
      "Account Type         object\n",
      "Author               object\n",
      "Impact              float64\n",
      "Impressions           int64\n",
      "Engagement Score    float64\n",
      "Interest             object\n",
      "dtype: object\n",
      "\n",
      "Basic statistical description:\n",
      "              Impact   Impressions  Engagement Score\n",
      "count  702699.000000  7.026990e+05     702699.000000\n",
      "mean        5.470749  1.801560e+04         14.103450\n",
      "std        12.923443  1.167940e+06       1101.380594\n",
      "min         0.000000  0.000000e+00          0.000000\n",
      "25%         0.000000  1.510000e+02          0.000000\n",
      "50%         0.000000  5.030000e+02          0.000000\n",
      "75%         1.500000  1.626000e+03          0.000000\n",
      "max        99.900000  8.449322e+08     356057.000000\n",
      "\n",
      "Missing value statistics:\n",
      "Query Name               0\n",
      "Date                     0\n",
      "Full Text                0\n",
      "Location Name       238622\n",
      "Account Type             0\n",
      "Author                   0\n",
      "Impact                   0\n",
      "Impressions              0\n",
      "Engagement Score         0\n",
      "Interest            444195\n",
      "dtype: int64\n",
      "\n",
      "Number of unique values per column:\n",
      "Query Name               2\n",
      "Date                555474\n",
      "Full Text           152779\n",
      "Location Name         2436\n",
      "Account Type             2\n",
      "Author              375985\n",
      "Impact                 995\n",
      "Impressions          36702\n",
      "Engagement Score      1403\n",
      "Interest              4396\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df_twitter = pd.read_csv('../../data/clean/twitter_data.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Data preview (first 5 rows):\")\n",
    "print(df_twitter.head())\n",
    "print(\"\\nData shape (rows, columns):\")\n",
    "print(df_twitter.shape)\n",
    "print(\"\\nColumn names:\")\n",
    "print(df_twitter.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(df_twitter.dtypes)\n",
    "print(\"\\nBasic statistical description:\")\n",
    "print(df_twitter.describe())\n",
    "print(\"\\nMissing value statistics:\")\n",
    "print(df_twitter.isnull().sum())\n",
    "\n",
    "# If you want to see the number of unique values in each column\n",
    "print(\"\\nNumber of unique values per column:\")\n",
    "print(df_twitter.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range from 2024-11-04 10:01:00 to 2025-01-04 09:58:16\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'Date' column to datetime type\n",
    "df_twitter['Date'] = pd.to_datetime(df_twitter['Date'])\n",
    "\n",
    "# Get the minimum and maximum dates\n",
    "min_date = df_twitter['Date'].min()\n",
    "max_date = df_twitter['Date'].max()\n",
    "\n",
    "print(f\"Date range from {min_date} to {max_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Sampling Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data rows: 702699\n",
      "Target sample size: 1000\n",
      "Actual sampled rows: 1000\n",
      "Sampling ratio: 0.14%\n",
      "Number of unique dates - original data: 62\n",
      "Number of unique dates - sampled data: 62\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed to 0 for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert Date to datetime format and extract the date part\n",
    "df_twitter['Date'] = pd.to_datetime(df_twitter['Date'])\n",
    "df_twitter['Date_only'] = df_twitter['Date'].dt.date\n",
    "\n",
    "# Set fixed target sample size to 1000\n",
    "target_sample_size = 1000\n",
    "\n",
    "# Get total rows and unique dates\n",
    "total_rows = len(df_twitter)\n",
    "unique_dates = df_twitter['Date_only'].unique()\n",
    "n_dates = len(unique_dates)\n",
    "\n",
    "# Step 1: Ensure at least 1 sample per date (minimum requirement)\n",
    "min_samples_total = n_dates  # At least 1 sample per date\n",
    "remaining_target = target_sample_size - min_samples_total  # Remaining samples to allocate proportionally\n",
    "\n",
    "# Stratified sampling\n",
    "sampled_df = pd.DataFrame()\n",
    "\n",
    "# Step 2: Calculate proportional samples per date, ensuring at least 1\n",
    "for date in unique_dates:\n",
    "    day_data = df_twitter[df_twitter['Date_only'] == date]\n",
    "    day_size = len(day_data)\n",
    "    \n",
    "    # Calculate proportional sample size based on date's share of total data\n",
    "    proportional_size = int((day_size / total_rows) * remaining_target)\n",
    "    \n",
    "    # Ensure at least 1 sample, then add proportional amount\n",
    "    samples_for_day = max(1, min(proportional_size + 1, day_size))\n",
    "    \n",
    "    # If calculated samples exceed available data, take all available\n",
    "    if samples_for_day > day_size:\n",
    "        sampled_day = day_data\n",
    "    else:\n",
    "        sampled_day = day_data.sample(n=samples_for_day, random_state=0)  # Use seed 0\n",
    "    \n",
    "    sampled_df = pd.concat([sampled_df, sampled_day])\n",
    "\n",
    "# Step 3: Adjust sample size to exactly 1000 if needed\n",
    "current_sample_size = len(sampled_df)\n",
    "if current_sample_size < target_sample_size:\n",
    "    # If under-sampled, add more from remaining data\n",
    "    remaining_df = df_twitter[~df_twitter.index.isin(sampled_df.index)]\n",
    "    additional_samples = remaining_df.sample(n=(target_sample_size - current_sample_size), \n",
    "                                           random_state=0)\n",
    "    sampled_df = pd.concat([sampled_df, additional_samples])\n",
    "elif current_sample_size > target_sample_size:\n",
    "    # If over-sampled, randomly reduce to 1000\n",
    "    sampled_df = sampled_df.sample(n=target_sample_size, random_state=0)\n",
    "\n",
    "# Drop temporary column\n",
    "sampled_df = sampled_df.drop('Date_only', axis=1)\n",
    "\n",
    "# Save the result\n",
    "sampled_df.to_csv('../../data/clean/twitter_sample.csv', index=False)\n",
    "\n",
    "# Verify the results\n",
    "print(f\"Original data rows: {total_rows}\")\n",
    "print(f\"Target sample size: {target_sample_size}\")\n",
    "print(f\"Actual sampled rows: {len(sampled_df)}\")\n",
    "print(f\"Sampling ratio: {len(sampled_df)/total_rows:.2%}\")\n",
    "print(f\"Number of unique dates - original data: {n_dates}\")\n",
    "print(f\"Number of unique dates - sampled data: {len(sampled_df['Date'].dt.date.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preview (first 5 rows):\n",
      "        id  type parent_id                                               text  \\\n",
      "0  1hq8tz0  post       NaN  More LM subs being mass censored by power hung...   \n",
      "1  1hpi74v  post       NaN  Prison inmates show solidarity with Luigi Mang...   \n",
      "2  1hphx7f  post       NaN  Censoring the Luigi Mangione information on Re...   \n",
      "3  1hphwjh  post       NaN             Luigi censorship: why they are afraid?   \n",
      "4  1hphwzw  post       NaN            The Luigi censorship; a running thread.   \n",
      "\n",
      "     author  subreddit  score  upvote_ratio  num_comments  \\\n",
      "0  libghost  FR33LUIGI      3          0.80           0.0   \n",
      "1  libghost  FR33LUIGI      3          0.71           0.0   \n",
      "2  libghost  FR33LUIGI      3          0.80           0.0   \n",
      "3  libghost  FR33LUIGI      3          1.00           0.0   \n",
      "4  libghost  FR33LUIGI      1          0.60           0.0   \n",
      "\n",
      "                                                 url  \n",
      "0  https://reddit.com/r/FR33LUIGI/comments/1hq8tz...  \n",
      "1  https://reddit.com/r/FR33LUIGI/comments/1hpi74...  \n",
      "2  https://reddit.com/r/FR33LUIGI/comments/1hphx7...  \n",
      "3  https://reddit.com/r/FR33LUIGI/comments/1hphwj...  \n",
      "4  https://reddit.com/r/FR33LUIGI/comments/1hphwz...  \n",
      "\n",
      "Data shape (rows, columns):\n",
      "(344823, 10)\n",
      "\n",
      "Column names:\n",
      "['id', 'type', 'parent_id', 'text', 'author', 'subreddit', 'score', 'upvote_ratio', 'num_comments', 'url']\n",
      "\n",
      "Data types:\n",
      "id               object\n",
      "type             object\n",
      "parent_id        object\n",
      "text             object\n",
      "author           object\n",
      "subreddit        object\n",
      "score             int64\n",
      "upvote_ratio    float64\n",
      "num_comments    float64\n",
      "url              object\n",
      "dtype: object\n",
      "\n",
      "Basic statistical description:\n",
      "               score  upvote_ratio  num_comments\n",
      "count  344823.000000  11355.000000  11355.000000\n",
      "mean       96.768844      0.871958    101.023954\n",
      "std      1590.042743      0.179848    431.373236\n",
      "min     -1089.000000      0.100000      0.000000\n",
      "25%         1.000000      0.840000      3.000000\n",
      "50%         2.000000      0.950000     14.000000\n",
      "75%        11.000000      0.990000     47.000000\n",
      "max    172321.000000      1.000000  10706.000000\n",
      "\n",
      "Missing value statistics:\n",
      "id                   0\n",
      "type                 0\n",
      "parent_id        11355\n",
      "text                 0\n",
      "author               0\n",
      "subreddit            0\n",
      "score                0\n",
      "upvote_ratio    333468\n",
      "num_comments    333468\n",
      "url             333468\n",
      "dtype: int64\n",
      "\n",
      "Number of unique values per column:\n",
      "id              268912\n",
      "type                 3\n",
      "parent_id        43461\n",
      "text            261885\n",
      "author          153550\n",
      "subreddit          365\n",
      "score             4006\n",
      "upvote_ratio        89\n",
      "num_comments       570\n",
      "url               4852\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df_reddit = pd.read_csv('../../data/clean/reddit_data.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Data preview (first 5 rows):\")\n",
    "print(df_reddit.head())\n",
    "print(\"\\nData shape (rows, columns):\")\n",
    "print(df_reddit.shape)\n",
    "print(\"\\nColumn names:\")\n",
    "print(df_reddit.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(df_reddit.dtypes)\n",
    "print(\"\\nBasic statistical description:\")\n",
    "print(df_reddit.describe())\n",
    "print(\"\\nMissing value statistics:\")\n",
    "print(df_reddit.isnull().sum())\n",
    "\n",
    "# If you want to see the number of unique values in each column\n",
    "print(\"\\nNumber of unique values per column:\")\n",
    "print(df_reddit.nunique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
